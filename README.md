# NimbusGuard - AI-Powered Kubernetes Autoscaling

An intelligent cloud-native autoscaling system that uses **Deep Q-Learning (DQN)** and **Large Language Model (LLM) validation** to make smart scaling decisions for Kubernetes workloads.

## ğŸ§  What Makes NimbusGuard Special?

Unlike traditional autoscaling that relies on simple CPU/memory thresholds, NimbusGuard uses:
- **ğŸ¤– AI Decision Engine**: DQN neural network trained on scientifically selected features
- **ğŸ›¡ï¸ LLM Safety Validator**: GPT-powered validation of scaling decisions with real cluster access
- **ğŸ§  Pure LLM Rewards**: Dynamic reward calculation using OpenAI GPT (no hardcoded thresholds)
- **ğŸ“Š Explainable AI**: Detailed reasoning for every scaling decision
- **ğŸ”„ Continuous Learning**: Real-time model improvement from intelligent feedback

---

## ğŸš€ Quick Setup (3 Commands)

### Prerequisites
- **Docker Desktop** with Kubernetes enabled
- **Git** installed
- **OpenAI API Key** (required - no fallbacks available)

### Clone Repository
```bash
# Clone with submodules (required)
git clone --recursive https://github.com/CSWanigasooriya/nimbusguard.git
cd nimbusguard

# If already cloned without --recursive:
git submodule update --init --recursive
```

### Deploy Everything
```bash
# 1. Setup tools and environment (auto-installs everything)
make setup

# 2. Build and deploy the complete AI system
make dev

# 3. Access services via port forwarding
make ports
```

**That's it!** ğŸ‰

---

## ğŸ¯ What Just Happened?

### `make setup` automatically:
- ğŸ”§ Installs kubectl, helm, docker (macOS/Linux detection)
- ğŸ“¦ Configures Helm repositories
- ğŸ”‘ **REQUIRED**: Sets up OpenAI API key (prompted during setup - mandatory for LLM rewards)
- ğŸ“Š Installs metrics-server for monitoring
- ğŸ—ï¸ Creates `nimbusguard` namespace

### `make dev` automatically:
- ğŸ”¨ Builds all Docker images
- ğŸ” Installs KEDA (if not present)
- ğŸš€ Deploys the complete AI-powered stack

### `make ports` provides access to:
- ğŸ“ˆ **Prometheus**: http://localhost:9090
- ğŸ“Š **Grafana**: http://localhost:3000 (admin/admin)
- ğŸ§  **DQN Adapter**: http://localhost:8080
- ğŸ’¾ **Redis**: redis-cli -p 6379
- ğŸ—„ï¸ **MinIO**: http://localhost:9000

---

## ğŸ§ª Test the AI System

```bash
# Light load test (gentle scaling)
make load-test-light

# Heavy load test (trigger AI scaling)
make load-test-heavy

# Check scaling status
make load-status

# Watch AI decisions in real-time
make logs-dqn
```

---

## ğŸ”§ Available Commands

| Command | Description |
|---------|-------------|
| `make setup` | Install all tools and configure environment |
| `make dev` | Build and deploy complete system |
| `make ports` | Port forward all services |
| `make load-test-*` | Run various load tests |
| `make logs-dqn` | Watch AI decision making |
| `make clean` | Nuclear cleanup (delete everything) |
| `make help` | Show all available commands |

---

## ğŸ› Troubleshooting

**If `make setup` fails:**
- Ensure Docker Desktop is running with Kubernetes enabled
- On Windows: Install `make` via `choco install make` or use PowerShell to run commands manually

**If deployment fails:**
- Run `make clean` then try `make dev` again
- **REQUIRED**: Ensure you have a valid OpenAI API key configured (system cannot function without it)

**View all available commands:**
```bash
make help
```

---

## ğŸ“ What Gets Deployed

- **ğŸ¤– DQN Adapter**: AI decision engine with intelligent reasoning
- **ğŸ›¡ï¸ LLM Validator**: GPT-powered safety validation with cluster access  
- **ğŸ“Š Consumer Service**: FastAPI application for load testing
- **âš–ï¸ KEDA Autoscaling**: Event-driven scaling based on AI decisions
- **ğŸ“ˆ Monitoring Stack**: Prometheus, Grafana, Alloy for observability
- **ğŸ” Auto-instrumentation**: Beyla for automatic metrics collection

**Watch the AI make intelligent scaling decisions in the logs!** ğŸ¤–

---

*Ready to experience the future of intelligent autoscaling! ğŸš€* 