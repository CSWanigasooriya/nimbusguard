# NimbusGuard: AI-Powered Kubernetes Resilience Platform

NimbusGuard is an AI-powered Kubernetes resilience platform that delivers proactive scaling and autonomous recovery using reinforcement learning and LangGraph workflows.

## Project Overview

NimbusGuard transforms traditional reactive Kubernetes scaling into intelligent, proactive resource management by:

- Learning from workload patterns using Q-learning reinforcement learning
- Predicting resource needs before demand spikes occur using LSTM models
- Making autonomous scaling decisions based on real-time cluster state
- Providing self-healing capabilities for scaling failures and anomalies
- Delivering comprehensive observability with full tracing and metrics

## Architecture Components

### Event-Driven Infrastructure

- **Kafka**: Event streaming platform for real-time data pipelines.
- **Load Generator**: Configurable service that generates realistic load. It can send HTTP traffic to the Consumer Workload App or trigger scaling decisions by sending events via REST or Kafka.
- **Consumer Workload Application**: A unified FastAPI application that both generates resource workloads (CPU/memory) and consumes scaling events from Kafka topics or a REST endpoint to trigger the AI decision engine.
- **KEDA**: Event-driven autoscaler that can scale the Consumer Workload App based on Kafka queue length or other metrics.

### Core AI Platform

- **LangGraph Operator**: Custom Kubernetes operator orchestrating a multi-agent AI system for decision-making.
- **Q-Learning Engine**: Reinforcement learning for dynamic scaling decisions with an epsilon-greedy exploration strategy.
- **LSTM Predictor**: Time series forecasting for proactive resource management.
- **Multi-Agent System**: Comprises a State Observer, Decision Agent, Action Executor, and Reward Calculator.

## Project Structure

```
nimbusguard/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ Makefile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ skaffold.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ consumer-workload/               # Unified FastAPI workload and event consumer app
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ main.py                      # FastAPI server entry point
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ workload_endpoints.py    # /workload/cpu, /memory
â”‚   â”‚   â”‚   â”œâ”€â”€ event_endpoints.py       # /events/trigger for REST-based scaling
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics_endpoints.py     # /metrics for Prometheus
â”‚   â”‚   â”‚   â””â”€â”€ health_endpoints.py      # /health, /ready
â”‚   â”‚   â”œâ”€â”€ consumers/                   # Kafka consumer logic
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ scaling_event_consumer.py # Consumes scaling events from Kafka
â”‚   â”‚   â”œâ”€â”€ workload_generators/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cpu_workload.py
â”‚   â”‚   â”‚   â””â”€â”€ memory_workload.py
â”‚   â”‚   â”œâ”€â”€ instrumentation/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ otel_config.py
â”‚   â”‚   â”‚   â””â”€â”€ metrics_collector.py
â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚       â””â”€â”€ app_config.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ load-generator/                  # Event-driven load generation service
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”‚   â”œâ”€â”€ http_load_generator.py
â”‚   â”‚   â”‚   â””â”€â”€ kafka_event_generator.py
â”‚   â”‚   â”‚  
â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚       â””â”€â”€ load_patterns.yaml
â”‚   â”‚
â”‚   â””â”€â”€ langgraph-operator/              # AI-powered Kubernetes operator
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ operator.py                  # Main operator controller
â”‚       â”œâ”€â”€ agents/                      # AI agent implementations
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ state_observer.py        # Cluster state observation agent
â”‚       â”‚   â”œâ”€â”€ decision_agent.py        # Q-learning scaling decisions
â”‚       â”‚   â”œâ”€â”€ action_executor.py       # Kubernetes scaling actions
â”‚       â”‚   â””â”€â”€ reward_calculator.py     # Reward calculation for RL
â”‚       â”œâ”€â”€ ml_models/                   # Machine learning models
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ q_learning.py            # Q-learning implementation
â”‚       â”‚   â”œâ”€â”€ lstm_predictor.py        # LSTM time series prediction
â”‚       â”‚   â””â”€â”€ anomaly_detector.py      # Statistical anomaly detection
â”‚       â”œâ”€â”€ workflows/                   # LangGraph workflow definitions
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ scaling_workflow.py      # Main scaling decision workflow
â”‚       â”‚   â”œâ”€â”€ monitoring_workflow.py   # Continuous monitoring workflow
â”‚       â”‚   â””â”€â”€ recovery_workflow.py     # Autonomous recovery workflow
â”‚       â”œâ”€â”€ mcp_integration/             # MCP server integration
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ mcp_manager.py           # MCP connection management
â”‚       â”‚   â”œâ”€â”€ kubernetes_client.py     # mcp-server-kubernetes integration
â”‚       â”‚   â””â”€â”€ prometheus_client.py     # mcp-server-prometheus integration
â”‚       â”œâ”€â”€ instrumentation/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ operator_metrics.py      # Operator-specific metrics
â”‚       â”‚   â”œâ”€â”€ decision_tracing.py      # AI decision tracing
â”‚       â”‚   â””â”€â”€ learning_metrics.py      # ML training metrics
â”‚       â””â”€â”€ config/
â”‚           â”œâ”€â”€ mcp_config.yaml          # MCP server configurations
â”‚           â”œâ”€â”€ agent_config.yaml        # AI agent settings
â”‚           â””â”€â”€ ml_config.yaml           # ML model hyperparameters
â”‚
â”œâ”€â”€ kubernetes-manifests/                # Kubernetes deployment manifests
â”‚   â”œâ”€â”€ base/                           # Base Kubernetes manifests
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ namespace.yaml              # nimbusguard namespace
â”‚   â”‚   â”œâ”€â”€ kafka.yaml                  # Kafka broker deployment + service
â”‚   â”‚   â”œâ”€â”€ zookeeper.yaml              # Zookeeper for Kafka
â”‚   â”‚   â”œâ”€â”€ consumer-workload.yaml      # Merged workload + consumer deployment
â”‚   â”‚   â”œâ”€â”€ load-generator.yaml         # Load generator deployment
â”‚   â”‚   â”œâ”€â”€ langgraph-operator.yaml     # LangGraph operator deployment
â”‚   â”‚   â”œâ”€â”€ mcp-kubernetes.yaml         # MCP Kubernetes server
â”‚   â”‚   â”œâ”€â”€ mcp-prometheus.yaml         # MCP Prometheus server
â”‚   â”‚   â””â”€â”€ rbac.yaml                   # RBAC permissions
â”‚   â””â”€â”€ components/                     # Additional components
â”‚       â”œâ”€â”€ observability/              # Observability stack
â”‚       â”‚   â”œâ”€â”€ kustomization.yaml
â”‚       â”‚   â”œâ”€â”€ prometheus.yaml
â”‚       â”‚   â”œâ”€â”€ grafana.yaml
â”‚       â”‚   â”œâ”€â”€ tempo.yaml
â”‚       â”‚   â”œâ”€â”€ loki.yaml
â”‚       â”‚   â””â”€â”€ opentelemetry.yaml
â”‚       â””â”€â”€ keda/                       # KEDA autoscaling
â”‚           â”œâ”€â”€ kustomization.yaml
â”‚           â”œâ”€â”€ keda-operator.yaml
â”‚           â””â”€â”€ scaled-objects.yaml
â”‚
â”œâ”€â”€ monitoring/                         # Monitoring configurations
â”‚   â”œâ”€â”€ alerts/
â”‚   â”‚   â”œâ”€â”€ nimbusguard-alerts.yaml     # Custom alerting rules
â”‚   â”‚   â”œâ”€â”€ scaling-alerts.yaml         # Scaling-specific alerts
â”‚   â”‚   â””â”€â”€ ai-model-alerts.yaml        # AI model performance alerts
â”‚   â”œâ”€â”€ dashboards/                     # Grafana dashboard JSON files
â”‚   â”‚   â”œâ”€â”€ nimbusguard-overview.json   # Main overview dashboard
â”‚   â”‚   â”œâ”€â”€ ai-decision-tracking.json   # AI decision analysis
â”‚   â”‚   â”œâ”€â”€ workload-performance.json   # Workload metrics dashboard
â”‚   â”‚   â””â”€â”€ learning-metrics.json       # ML model performance
â”‚   â””â”€â”€ queries/                        # PromQL query examples
â”‚       â”œâ”€â”€ scaling-metrics.promql      # Scaling-related queries
â”‚       â”œâ”€â”€ ai-performance.promql       # AI performance queries
â”‚       â””â”€â”€ workload-analysis.promql    # Workload analysis queries
â”‚
â”œâ”€â”€ scripts/                            # Utility and deployment scripts
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ setup-local-cluster.sh      # Kind cluster setup
â”‚   â”‚   â”œâ”€â”€ verify-cluster-context.sh   # Verify kubectl context
â”‚   â”‚   â”œâ”€â”€ install-dependencies.sh     # Install required components
â”‚   â”‚   â”œâ”€â”€ deploy-observability.sh     # Deploy monitoring stack
â”‚   â”‚   â””â”€â”€ setup-development.sh        # Complete dev environment
â”‚   â”œâ”€â”€ operations/
â”‚   â”‚   â”œâ”€â”€ deploy-nimbusguard.sh       # Deploy main platform
â”‚   â”‚   â”œâ”€â”€ deploy-argocd.sh            # Deploy ArgoCD for GitOps (Phase 2)
â”‚   â”‚   â””â”€â”€ health-check.sh             # System health validation
â”‚   â””â”€â”€ utilities/
â”‚       â”œâ”€â”€ port-forward.sh             # Port forwarding for development
â”‚       â”œâ”€â”€ logs.sh                     # Centralized log collection
â”‚       â””â”€â”€ cleanup.sh                  # Resource cleanup
â”‚
â”œâ”€â”€ ml-training/                        # Machine learning pipeline
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ training-data/              # Historical training datasets
â”‚   â”‚   â””â”€â”€ model-artifacts/            # Trained model files
â”‚   â”œâ”€â”€ notebooks/                      # Jupyter analysis notebooks
â”‚   â”‚   â”œâ”€â”€ data-exploration.ipynb
â”‚   â”‚   â”œâ”€â”€ q-learning-analysis.ipynb
â”‚   â”‚   â””â”€â”€ lstm-development.ipynb
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ train-q-learning.py
â”‚       â””â”€â”€ train-lstm.py
â”‚
â”œâ”€â”€ tests/                              # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_consumer_workload.py
â”‚   â”‚   â””â”€â”€ test_q_learning.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_scaling_workflow.py
â”‚   â”‚   â””â”€â”€ test_ai_agents.py
â”‚   â””â”€â”€ e2e/
â”‚       â””â”€â”€ test_complete_scaling.py
â”‚
â””â”€â”€ .github/                            # CI/CD workflows
    â””â”€â”€ workflows/
        â”œâ”€â”€ ci.yaml                     # Continuous Integration
        â”œâ”€â”€ cd.yaml                     # Continuous Deployment
        â””â”€â”€ ml-training.yaml            # ML model training pipeline
```

## Additional ML Platform Integration

### MLflow Integration (Phase 1) - Add to src/langgraph-operator/mlflow_integration/
```
â”œâ”€â”€ __init__.py
â”œâ”€â”€ experiment_tracker.py    # MLflow experiment management
â”œâ”€â”€ model_registry.py        # Model versioning and registry
â””â”€â”€ artifact_logger.py       # Log training artifacts and metrics
```

### ArgoCD & Kubeflow Components (Phase 2/3) - Add to kubernetes-manifests/components/argocd/
```
â”œâ”€â”€ kustomization.yaml
â”œâ”€â”€ argocd-server.yaml
â””â”€â”€ argocd-application.yaml
```

### kubeflow/ (Phase 3 - Optional)
```
â”œâ”€â”€ kustomization.yaml
â””â”€â”€ kubeflow-pipelines.yaml
```

## Development Roadmap & Implementation Phases

The project is designed to be implemented in three distinct phases, starting with a foundational AI system and progressively adding advanced capabilities.

### Phase 1: Foundation & Core AI (Current Priority)

This phase focuses on building the essential components for a functional AI-driven scaling system.

#### Infrastructure Setup:
âœ… Deploy a baseline observability stack (Prometheus, Grafana).
âœ… Install KEDA for event-driven autoscaling.
âœ… Configure Kafka and Zookeeper for event streaming.
âœ… Set up basic Kubernetes RBAC and custom resource definitions.

#### Core Application Development:
ðŸ”¥ Unified Consumer Workload App: Develop the FastAPI application that both serves a resource-intensive workload and consumes scaling events via REST and Kafka.
ðŸ”¥ Load Generator Service: Create a service to generate configurable HTTP load and trigger scaling events.

#### AI & Operator Development:
ðŸ”¥ LangGraph Operator: Build the initial Kubernetes operator.
ðŸ”¥ Basic Q-Learning Agent: Implement a Q-learning agent with a simple state-action space for decision-making.
ðŸ”¥ Core Agent Workflow: Create a single-agent LangGraph workflow (State Observer -> Decision Agent -> Action Executor).

#### ML Lifecycle:
ðŸ”¥ MLflow Integration: Integrate MLflow for experiment tracking, model registry, and artifact logging.

### Phase 2: Advanced AI & GitOps Automation (Future Work)

This phase enhances the AI's intelligence, predictive power, and automates the deployment lifecycle using GitOps.

#### GitOps & Deployment Automation:
ðŸ”® ArgoCD Integration: Implement GitOps for continuous deployment of applications and AI model configurations.
ðŸ”® Automated Model Deployment: Build CI/CD pipelines in GitHub Actions to automatically train and deploy new models via ArgoCD.

#### Advanced AI Models:
ðŸ”® Deep Q-Networks (DQN): Upgrade the reinforcement learning model from tabular Q-learning to a neural network-based DQN for handling more complex state spaces.
ðŸ”® LSTM Time Series Prediction: Develop and integrate a 3-layer LSTM model for proactive, 5-minute-ahead resource prediction.
ðŸ”® Advanced Anomaly Detection: Implement statistical and ML-based outlier detection to identify and react to unusual workload patterns.

#### Enhanced Intelligence & Workflows:
ðŸ”® Predictive Scaling: Combine LSTM predictions with Q-learning decisions for a hybrid proactive-reactive scaling strategy.
ðŸ”® Multi-Agent Coordination: Evolve the LangGraph workflow to support sophisticated communication and decision-making between multiple specialized agents.

### Phase 3: Production Hardening & Enterprise Scale (Long-Term Vision)

This phase focuses on making the platform robust, self-sufficient, and ready for production-grade, enterprise environments.

#### Enterprise & Production Features:
ðŸ”® Self-Healing Workflows: Design autonomous recovery workflows in LangGraph to handle scaling failures, pod crashes, or infrastructure anomalies.
ðŸ”® Policy Adaptation & Auto-Tuning: Introduce real-time hyperparameter tuning for the AI models based on performance feedback.
ðŸ”® Multi-Cluster Federation: Extend the operator's capability to manage and learn from workloads across multiple Kubernetes clusters.

#### Advanced MLOps (Optional Upgrade):
ðŸ”® Kubeflow Migration: Optionally migrate the ML lifecycle management from MLflow to Kubeflow to leverage its powerful pipeline orchestration and distributed training capabilities for enterprise-scale needs.

#### Research & Validation:
ðŸ”® Performance Benchmarking: Conduct rigorous A/B testing to compare the performance and cost-efficiency of NimbusGuard against traditional HPA/VPA.
ðŸ”® Publish Findings: Author and publish a research paper detailing the novel reinforcement learning approach to Kubernetes autoscaling.

**Legend:**
âœ… = Existing infrastructure to leverage.
ðŸ”¥ = Current phase custom implementation (novel research).
ðŸ”® = Future phase implementation.

## Quick Start

### Prerequisites
- Docker Desktop with Kubernetes enabled
- Python 3.11+
- kubectl configured for your local cluster
- Git & Helm 3.x

### Setup and Deployment
```bash
# Clone the repository
git clone https://github.com/chamathwanigasooriya/nimbusguard.git
cd nimbusguard

# Deploy with hot-reload for development
skaffold dev
```

### Accessing Services
```bash
# Port forward all services
./scripts/utilities/port-forward.sh

# Access points:
# - Grafana: http://localhost:3000
# - Prometheus: http://localhost:9090
# - Consumer Workload App: http://localhost:8080
# - Load Generator: http://localhost:8081
# - MLflow UI: http://localhost:5000
```

### Testing Scaling Scenarios
```bash
# 1. Generate a direct CPU workload on the consumer app
curl -X POST "http://localhost:8080/workload/cpu?intensity=80&duration=300"

# 2. Trigger a scaling event via the consumer app's REST endpoint
curl -X POST "http://localhost:8080/events/trigger" \
  -H "Content-Type: application/json" \
  -d '{
    "event_type": "high_cpu_usage",
    "service": "consumer-workload",
    "value": 90
  }'

# 3. Use the Load Generator to create a sustained event stream (e.g., to Kafka)
curl -X POST "http://localhost:8081/load/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "pattern": "spike",
    "duration": 300,
    "target": "kafka"
  }'

# Monitor the AI operator's decisions
kubectl logs -f deployment/langgraph-operator -n nimbusguard
```

## AI Agents Implementation

### State Observer Agent
- Collects real-time cluster metrics via MCP Prometheus server.
- Monitors pod CPU, memory, network I/O, and request rates.
- Aggregates node-level resource availability and utilization.
- Provides normalized state vectors for the Q-learning agent.

### Decision Agent (Q-Learning)
- Implements epsilon-greedy Q-learning with learning rate 0.1.
- State space: CPU utilization, memory utilization, request rate, pod count.
- Action space: scale up (1-10 replicas), scale down (1-10 replicas), no action.
- Reward function: -1 for resource waste, +10 for optimal utilization, -5 for SLA violations.

### LSTM Predictor Agent
- Uses 3-layer LSTM with 128 hidden units per layer.
- Predicts CPU and memory utilization 5 minutes ahead.
- Trained on historical metrics with a 60-minute sliding window.
- Provides proactive scaling recommendations to Decision Agent.

### Action Executor Agent
- Executes scaling decisions via Kubernetes API.
- Updates KEDA ScaledObjects for event-driven scaling.
- Modifies HPA configurations for traditional horizontal scaling.
- Records action outcomes for reward calculation.

### Reward Calculator Agent
- Calculates rewards based on: resource efficiency (40%), SLA compliance (40%), stability (20%).
- Tracks scaling effectiveness over 10-minute windows.
- Provides feedback for Q-learning policy updates.
- Logs reward signals for performance analysis.

## Observability Features

### Metrics Collection
- Workload Metrics: CPU/memory usage, request latency, throughput.
- AI Decision Metrics: Decision confidence, action success rate, learning progress.
- Scaling Metrics: Scaling events, replica changes, resource utilization.
- System Metrics: Overall platform health, error rates, performance.

### Tracing
- End-to-end tracing of scaling decisions through LangGraph workflows.
- AI agent interaction spans with decision rationale.
- MCP server communication tracing.
- Performance bottleneck identification.

### Dashboards
- NimbusGuard Overview: Platform status, scaling events, resource utilization.
- AI Decision Tracking: Real-time AI decisions, confidence scores, learning curves.
- Workload Performance: Application metrics, response times, error rates.
- Learning Metrics: Q-learning performance, LSTM accuracy, exploration rates.

## Development Commands

```bash
# Development workflow
make dev                # Start development with Skaffold
make test               # Run full test suite
make lint               # Run code linting and formatting
make build              # Build Docker images locally

# Deployment
make deploy             # Deploy base platform
make deploy-full        # Deploy with observability and KEDA
make clean              # Clean up all resources

# Monitoring
make logs               # Tail application logs
make metrics            # Open Prometheus metrics
make dashboard          # Open Grafana dashboards
make health             # Check system health

# AI/ML Development
make train-models       # Train Q-learning and LSTM models
make validate-models    # Run model validation tests
make export-data        # Export training data for analysis

# MLflow Operations (Phase 1)
make deploy-mlflow      # Deploy MLflow tracking server
make mlflow-ui          # Open MLflow web UI
make track-experiment   # Start experiment tracking

# ArgoCD Operations (Phase 2)
make deploy-argocd      # Deploy ArgoCD server
make argocd-ui          # Open ArgoCD dashboard
```

## Event-Driven Architecture Flow

With the unified application, the event flow is more direct:

```
Load Generator â†’ (REST API / Kafka Events) â†’ Consumer Workload App â†’ AI Decision Engine
       â†“                                           â†“                    â†“
  HTTP Requests -----------------------------------â†’ (Generates Metrics)    â†“
                                                     â†“                    â†“
                                                 Prometheus â†’    MCP    â†’ State Observer
```

### Event Flow Steps:
1. The Load Generator produces events. It can send HTTP traffic directly to the Consumer Workload App to create resource pressure, or it can send a scaling trigger event via REST or Kafka.
2. The Consumer Workload App receives the trigger event on its /events/trigger endpoint or via its integrated Kafka consumer.
3. Upon receiving an event, the app notifies the LangGraph Operator to initiate a scaling analysis.
4. The AI agents within the operator observe cluster metrics via Prometheus, make a decision, and execute a scaling action.
5. Metrics from the action's outcome are fed back into the system, allowing the AI to learn continuously.

## Technology Stack

### ML Platform Choice: MLflow vs Kubeflow

#### Primary Choice: MLflow (Phase 1)
âœ… Lightweight: Minimal infrastructure overhead, perfect for research projects.
âœ… Easy Setup: Simple deployment and management.
âœ… Research Focused: Excellent for experimentation and model development.
âœ… Local Development: Works well with Docker Desktop and Kind clusters.

#### Optional Upgrade: Kubeflow (Phase 3)
ðŸ”® Enterprise Scale: Complex ML workflows for production environments.
ðŸ”® Distributed Training: Multi-node training for large models.
ðŸ”® Advanced Pipelines: Sophisticated ML orchestration.

**Recommendation:** Start with MLflow for development and research, optionally migrate to Kubeflow if enterprise-scale features are needed.

### Core Technologies
- Python 3.11
- FastAPI
- LangGraph
- Kubernetes 1.28+
- Docker

### Event Streaming & Messaging
- Apache Kafka
- kafka-python

### AI/ML Libraries
- NumPy & Pandas
- Scikit-learn
- TensorFlow 2.x (Phase 2)
- Gymnasium

### ML Platform & Lifecycle Management
- MLflow (Primary Choice)
- Jupyter
- Kubeflow (Optional Upgrade)

### GitOps & Deployment
- ArgoCD (Phase 2)
- Helm
- Kustomize

### Observability
- Prometheus, Grafana, Tempo, Loki, OpenTelemetry

### Infrastructure
- KEDA, Kubernetes HPA, MCP

## Configuration

### Q-Learning Parameters
```
learning_rate: 0.1
discount_factor: 0.95
epsilon_start: 1.0
epsilon_end: 0.01
epsilon_decay: 0.995
```

### LSTM Model Configuration
```
sequence_length: 60
hidden_units: 128
num_layers: 3
dropout_rate: 0.2
```

### Scaling Parameters
```
min_replicas: 1
max_replicas: 50
cooldown_period: 300
```

## License

This project is licensed under the MIT License.

## Related Projects

- [KEDA](https://keda.sh/)
- [LangGraph](https://github.com/langchain-ai/langgraph)
- [Model Context Protocol](https://github.com/modelcontextprotocol/mcp)
- [Prometheus](https://prometheus.io/)
- [MLflow](https://mlflow.org/)
- [ArgoCD](https://argoproj.github.io/cd/)
- [Kubeflow](https://www.kubeflow.org/)