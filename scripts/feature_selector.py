#!/usr/bin/env python3
"""
Advanced DQN Feature Engineering for Kubernetes Pod Autoscaling
==============================================================

This script implements research-grade feature engineering using advanced statistical
and machine learning techniques to extract the 11 most critical features for DQN
pod scaling decisions.

Research Methods Applied:
1. Principal Component Analysis (PCA) for dimensionality reduction
2. Mutual Information for feature selection
3. Statistical significance testing
4. Time-series decomposition
5. Correlation analysis with target variable
6. Domain knowledge integration

Target: 11 optimal features for DQN state representation
"""

import pandas as pd
import numpy as np
import joblib
import requests
from pathlib import Path
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Advanced ML and statistical libraries
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import (
    mutual_info_regression, 
    SelectKBest, 
    f_regression,
    RFE
)
from sklearn.ensemble import RandomForestRegressor
from scipy import stats
from scipy.stats import pearsonr, spearmanr
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.stats.diagnostic import acorr_ljungbox

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

class AdvancedDQNFeatureEngineer:
    """
    Advanced feature engineering for DQN pod scaling using statistical methods.
    
    Implements research-grade techniques to extract the 11 most predictive features
    for Kubernetes pod autoscaling decisions with dynamic metric discovery.
    """
    
    def __init__(self, data_dir: Path, target_features: int = 8, prometheus_url: str = "http://localhost:9090"):
        self.data_dir = Path(data_dir)
        self.target_features = target_features
        self.prometheus_url = prometheus_url
        self.logger = logging.getLogger(__name__)
        
        # Consumer pod metric patterns - dynamically discovered
        self.consumer_metric_patterns = [
            # HTTP/Request metrics (critical for load-based scaling)
            'http_request',
            'http_response',
            'http_duration',
            'request_',
            'response_',
            
            # Application-specific metrics
            'consumer_',
            'processing_',
            'queue_',
            'worker_',
            
            # Resource utilization from consumer pod
            'process_cpu',
            'process_memory',
            'process_resident_memory',
            'go_memstats',
            'go_goroutines',
            
            # System health indicators
            'up',
            'scrape_duration',
            
            # Kubernetes pod metrics
            'kube_pod_',
            'kube_deployment_',
            'container_',
            
            # Network and I/O
            'network_',
            'io_',
            'disk_'
        ]
        
        # Statistical parameters
        self.confidence_level = 0.95
        self.significance_threshold = 0.05
        
    def discover_available_metrics(self) -> List[str]:
        """Discover available metrics from Prometheus API."""
        self.logger.info("ðŸ” Discovering available metrics from Prometheus...")
        
        try:
            # Query Prometheus API for all available metric names
            response = requests.get(
                f"{self.prometheus_url}/api/v1/label/__name__/values",
                timeout=10
            )
            response.raise_for_status()
            
            data = response.json()
            if data['status'] != 'success':
                raise ValueError(f"Prometheus API error: {data.get('error', 'Unknown error')}")
            
            all_metrics = data['data']
            self.logger.info(f"  âœ… Found {len(all_metrics)} total metrics in Prometheus")
            
            return all_metrics
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"  âŒ Failed to connect to Prometheus: {e}")
            self.logger.warning("  âš ï¸ Falling back to CSV file discovery...")
            return self._discover_metrics_from_files()
        except Exception as e:
            self.logger.error(f"  âŒ Error querying Prometheus: {e}")
            self.logger.warning("  âš ï¸ Falling back to CSV file discovery...")
            return self._discover_metrics_from_files()
    
    def _discover_metrics_from_files(self) -> List[str]:
        """Fallback: discover metrics from existing CSV files."""
        csv_files = list(self.data_dir.glob("*.csv"))
        metrics = [f.stem for f in csv_files]
        self.logger.info(f"  ðŸ“ Found {len(metrics)} metrics from CSV files")
        return metrics
    
    def filter_consumer_metrics(self, all_metrics: List[str]) -> Dict[str, List[str]]:
        """Filter metrics to focus on consumer pod relevant metrics."""
        self.logger.info("ðŸŽ¯ Filtering metrics for consumer pod focus...")
        
        filtered_metrics = {
            'load_metrics': [],
            'resource_metrics': [],
            'health_metrics': [],
            'application_metrics': [],
            'system_metrics': []
        }
        
        # Define metric categorization patterns
        categorization_rules = {
            'load_metrics': [
                'http_request', 'http_response', 'http_duration', 'request_', 'response_',
                'rpc_', 'grpc_', 'api_', 'endpoint_'
            ],
            'resource_metrics': [
                'process_cpu', 'process_memory', 'process_resident_memory', 'go_memstats',
                'go_goroutines', 'container_cpu', 'container_memory', 'memory_usage'
            ],
            'health_metrics': [
                'up', 'scrape_duration', 'kube_pod_status', 'kube_deployment_status',
                'container_last_seen', 'probe_', 'health_'
            ],
            'application_metrics': [
                'consumer_', 'processing_', 'queue_', 'worker_', 'job_', 'task_',
                'message_', 'event_', 'batch_'
            ],
            'system_metrics': [
                'node_', 'network_', 'disk_', 'filesystem_', 'io_'
            ]
        }
        
        # Filter and categorize metrics
        for metric in all_metrics:
            metric_lower = metric.lower()
            
            # Check if metric matches consumer patterns
            is_consumer_relevant = any(
                pattern.lower() in metric_lower 
                for pattern in self.consumer_metric_patterns
            )
            
            if is_consumer_relevant:
                # Categorize the metric
                categorized = False
                for category, patterns in categorization_rules.items():
                    if any(pattern.lower() in metric_lower for pattern in patterns):
                        filtered_metrics[category].append(metric)
                        categorized = True
                        break
                
                # If not categorized, add to application metrics as default
                if not categorized:
                    filtered_metrics['application_metrics'].append(metric)
        
        # Log results
        total_filtered = sum(len(metrics) for metrics in filtered_metrics.values())
        self.logger.info(f"  âœ… Filtered to {total_filtered} consumer-relevant metrics:")
        for category, metrics in filtered_metrics.items():
            if metrics:
                self.logger.info(f"    ðŸ“Š {category}: {len(metrics)} metrics")
                # Show first few metrics as examples
                examples = metrics[:3]
                if len(metrics) > 3:
                    examples.append(f"... (+{len(metrics)-3} more)")
                self.logger.info(f"      Examples: {', '.join(examples)}")
        
        return filtered_metrics
    
    def validate_metric_availability(self, metrics_dict: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """Validate that discovered metrics have corresponding CSV files."""
        self.logger.info("âœ… Validating metric data availability...")
        
        validated_metrics = {category: [] for category in metrics_dict.keys()}
        missing_files = []
        
        for category, metrics in metrics_dict.items():
            for metric in metrics:
                csv_file = self.data_dir / f"{metric}.csv"
                if csv_file.exists():
                    # Quick validation - check if file has data
                    try:
                        df = pd.read_csv(csv_file, nrows=1)
                        if len(df) > 0 and 'value' in df.columns:
                            validated_metrics[category].append(metric)
                        else:
                            missing_files.append(f"{metric} (empty or invalid format)")
                    except Exception:
                        missing_files.append(f"{metric} (read error)")
                else:
                    missing_files.append(f"{metric} (file not found)")
        
        # Log validation results
        total_validated = sum(len(metrics) for metrics in validated_metrics.values())
        self.logger.info(f"  âœ… Validated {total_validated} metrics with available data")
        
        if missing_files:
            self.logger.warning(f"  âš ï¸ {len(missing_files)} metrics missing data:")
            for missing in missing_files[:10]:  # Show first 10
                self.logger.warning(f"    - {missing}")
            if len(missing_files) > 10:
                self.logger.warning(f"    ... and {len(missing_files) - 10} more")
        
        return validated_metrics
    
    def _filter_consumer_pod_data(self, df: pd.DataFrame, metric_name: str) -> pd.DataFrame:
        """Filter CSV data to only include consumer pod metrics (matching DQN adapter logic)."""
        if 'job' not in df.columns or 'instance' not in df.columns:
            # If no job/instance columns, return all data (fallback)
            return df
        
        # Define consumer pod filters (matching main.py logic)
        consumer_filters = [
            # Primary: consumer pod job
            df['job'] == 'prometheus.scrape.nimbusguard_consumer',
            # Secondary: consumer instance
            df['instance'] == 'consumer:8000',
            # Tertiary: any port 8000 instance (consumer app port)
            df['instance'].str.contains(':8000', na=False),
        ]
        
        # Try filters in order of preference
        for filter_condition in consumer_filters:
            filtered_df = df[filter_condition]
            if len(filtered_df) > 0:
                self.logger.debug(f"    ðŸ“Š {metric_name}: Using {filter_condition.name if hasattr(filter_condition, 'name') else 'consumer filter'} -> {len(filtered_df)} rows")
                return filtered_df
        
        # If no consumer-specific data found, check for special cases
        if metric_name.startswith('node_') or metric_name.startswith('kube_'):
            # For node/kube metrics, we still want them (they're system-wide)
            return df
        
        # For other metrics, return empty to exclude non-consumer data
        self.logger.debug(f"    âš ï¸ {metric_name}: No consumer pod data found, excluding")
        return pd.DataFrame()

    def load_and_prepare_data(self) -> pd.DataFrame:
        """Load and prepare data with dynamic metric discovery and validation."""
        self.logger.info("ðŸ”¬ Loading data with dynamic metric discovery...")
        
        # Step 1: Discover available metrics
        all_metrics = self.discover_available_metrics()
        
        # Step 2: Filter for consumer-relevant metrics
        consumer_metrics = self.filter_consumer_metrics(all_metrics)
        
        # Step 3: Validate data availability
        validated_metrics = self.validate_metric_availability(consumer_metrics)
        
        # Step 4: Load validated metrics
        combined_data = []
        data_quality_report = {
            'loaded_metrics': 0,
            'failed_metrics': 0,
            'total_rows': 0,
            'missing_data_percentage': 0
        }
        
        for category, metrics in validated_metrics.items():
            if not metrics:
                continue
                
            self.logger.info(f"Loading {category} ({len(metrics)} metrics)...")
            
            for metric in metrics:
                csv_file = self.data_dir / f"{metric}.csv"
                try:
                    df = pd.read_csv(csv_file)
                    
                    # Data quality checks
                    if len(df) == 0:
                        self.logger.warning(f"  âš ï¸ Empty dataset: {metric}")
                        continue
                        
                    # Check for required columns
                    required_cols = ['timestamp', 'value']
                    if not all(col in df.columns for col in required_cols):
                        self.logger.warning(f"  âš ï¸ Missing required columns: {metric}")
                        continue
                    
                    # Filter to consumer pod data only (matching DQN adapter queries)
                    df_filtered = self._filter_consumer_pod_data(df, metric)
                    if len(df_filtered) == 0:
                        self.logger.warning(f"  âš ï¸ No consumer pod data found for: {metric}")
                        continue
                    
                    df_filtered['metric_name'] = metric
                    df_filtered['category'] = category
                    combined_data.append(df_filtered)
                    data_quality_report['loaded_metrics'] += 1
                    data_quality_report['total_rows'] += len(df_filtered)
                    
                    self.logger.debug(f"  âœ… {metric}: {len(df)} rows")
                    
                except Exception as e:
                    self.logger.error(f"  âŒ Failed to load {metric}: {e}")
                    data_quality_report['failed_metrics'] += 1
        
        if not combined_data:
            raise ValueError("No valid consumer metrics data could be loaded!")
        
        # Combine and clean data
        df = pd.concat(combined_data, ignore_index=True)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # Calculate missing data percentage
        data_quality_report['missing_data_percentage'] = (df['value'].isnull().sum() / len(df)) * 100
        
        # Store the discovered metrics for later use
        self.discovered_metrics = validated_metrics
        
        self.logger.info(f"ðŸ“Š Data Quality Report (Consumer Metrics Focus):")
        self.logger.info(f"  - Loaded metrics: {data_quality_report['loaded_metrics']}")
        self.logger.info(f"  - Failed metrics: {data_quality_report['failed_metrics']}")
        self.logger.info(f"  - Total rows: {data_quality_report['total_rows']:,}")
        self.logger.info(f"  - Missing data: {data_quality_report['missing_data_percentage']:.2f}%")
        
        return df
    
    def create_time_series_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create sophisticated time-series features using high-resolution sampling."""
        self.logger.info("â° Creating advanced time-series features with increased sampling...")
        
        # Instead of grouping by timestamp (which reduces to ~894 samples),
        # we'll resample to a higher frequency to get more training data
        
        # First, pivot to time-series format but keep more granular timestamps
        ts_df = df.groupby(['timestamp', 'metric_name'])['value'].mean().reset_index()
        ts_df = ts_df.pivot(index='timestamp', columns='metric_name', values='value')
        ts_df = ts_df.sort_index()
        
        # Keep the natural 1-minute sampling frequency - don't artificially interpolate
        # For per-minute Prometheus data, we should respect the natural sampling rate
        # ts_df = ts_df.resample('30S').mean()  # REMOVED: This creates artificial data
        
        # Forward fill and backward fill to handle missing data
        ts_df = ts_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # Add temporal features
        ts_df['hour'] = ts_df.index.hour
        ts_df['day_of_week'] = ts_df.index.dayofweek
        ts_df['is_business_hours'] = ts_df['hour'].between(9, 17).astype(int)
        
        # Cyclical encoding (advanced approach)
        ts_df['hour_sin'] = np.sin(2 * np.pi * ts_df['hour'] / 24)
        ts_df['hour_cos'] = np.cos(2 * np.pi * ts_df['hour'] / 24)
        ts_df['day_sin'] = np.sin(2 * np.pi * ts_df['day_of_week'] / 7)
        ts_df['day_cos'] = np.cos(2 * np.pi * ts_df['day_of_week'] / 7)
        
        self.logger.info(f"  âœ… Created time-series dataset: {ts_df.shape}")
        self.logger.info(f"      ðŸ“Š Using natural per-minute sampling: {len(ts_df)} samples")
        return ts_df
    
    def engineer_domain_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Engineer domain-specific features based on autoscaling theory."""
        self.logger.info("ðŸŽ¯ Engineering domain-specific autoscaling features...")
        
        feature_df = df.copy()
        
        # 1. Request Rate (critical for load-based scaling)
        if 'http_requests_total' in feature_df.columns:
            feature_df['request_rate'] = feature_df['http_requests_total'].diff() / 60  # per minute
            feature_df['request_rate'] = feature_df['request_rate'].fillna(0).clip(lower=0)
            
            # Request rate velocity (acceleration)
            feature_df['request_rate_velocity'] = feature_df['request_rate'].diff()
            feature_df['request_rate_velocity'] = feature_df['request_rate_velocity'].fillna(0)
        
        # 2. Response Time (latency-based scaling signal)
        if all(col in feature_df.columns for col in ['http_request_duration_seconds_sum', 'http_request_duration_seconds_count']):
            feature_df['avg_response_time'] = (
                feature_df['http_request_duration_seconds_sum'] / 
                (feature_df['http_request_duration_seconds_count'] + 1e-6)
            ) * 1000  # Convert to milliseconds
            
            # Response time percentiles using rolling statistics (20-minute window)
            feature_df['response_time_p95'] = feature_df['avg_response_time'].rolling(
                window=20, min_periods=1
            ).quantile(0.95)
        
        # 3. Current Replica Count (state variable)
        if 'kube_deployment_status_replicas' in feature_df.columns:
            feature_df['current_replicas'] = feature_df['kube_deployment_status_replicas']
            
            # Replica utilization efficiency
            if 'request_rate' in feature_df.columns:
                feature_df['requests_per_replica'] = (
                    feature_df['request_rate'] / (feature_df['current_replicas'] + 1e-6)
                )
        
        # 4. Resource Utilization Rate
        if 'alloy_resources_process_cpu_seconds_total' in feature_df.columns:
            feature_df['cpu_utilization_rate'] = feature_df['alloy_resources_process_cpu_seconds_total'].diff() / 60
            feature_df['cpu_utilization_rate'] = feature_df['cpu_utilization_rate'].fillna(0).clip(lower=0)
        
        # 5. Memory Pressure (use consumer pod memory, matching DQN adapter)
        if 'process_resident_memory_bytes' in feature_df.columns:
            feature_df['memory_usage_mb'] = feature_df['process_resident_memory_bytes'] / (1024 * 1024)
            
            # Memory growth rate
            feature_df['memory_growth_rate'] = feature_df['memory_usage_mb'].pct_change()
            feature_df['memory_growth_rate'] = feature_df['memory_growth_rate'].fillna(0)
        elif 'alloy_resources_process_resident_memory_bytes' in feature_df.columns:
            # Fallback to alloy memory if consumer memory not available
            feature_df['memory_usage_mb'] = feature_df['alloy_resources_process_resident_memory_bytes'] / (1024 * 1024)
            feature_df['memory_growth_rate'] = feature_df['memory_usage_mb'].pct_change()
            feature_df['memory_growth_rate'] = feature_df['memory_growth_rate'].fillna(0)
        
        # 6. System Health Score
        if all(col in feature_df.columns for col in ['kube_deployment_status_replicas_available', 'kube_deployment_status_replicas']):
            feature_df['health_ratio'] = (
                feature_df['kube_deployment_status_replicas_available'] / 
                (feature_df['kube_deployment_status_replicas'] + 1e-6)
            )
        
        # 7. Concurrency Level
        if 'go_goroutines' in feature_df.columns:
            feature_df['concurrency_level'] = feature_df['go_goroutines']
            
            # Concurrency per replica
            if 'current_replicas' in feature_df.columns:
                feature_df['concurrency_per_replica'] = (
                    feature_df['concurrency_level'] / (feature_df['current_replicas'] + 1e-6)
                )
        
        # 8. System Stability Indicators
        feature_df['system_stability'] = feature_df['up'].rolling(window=10, min_periods=1).mean()
        
        self.logger.info(f"  âœ… Engineered domain features: {feature_df.shape[1]} total columns")
        return feature_df
    
    def apply_statistical_transformations(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply advanced statistical transformations for better feature quality."""
        self.logger.info("ðŸ“ˆ Applying statistical transformations...")
        
        feature_df = df.copy()
        numeric_cols = feature_df.select_dtypes(include=[np.number]).columns
        
        # Remove timestamp and other non-feature columns
        feature_cols = [col for col in numeric_cols if col not in ['hour', 'day_of_week']]
        
        for col in feature_cols:
            if feature_df[col].std() > 0:  # Only transform non-constant features
                
                # 1. Log transformation for skewed distributions
                if feature_df[col].min() > 0:  # Only for positive values
                    skewness = stats.skew(feature_df[col].dropna())
                    if abs(skewness) > 1:  # Highly skewed
                        feature_df[f'{col}_log'] = np.log1p(feature_df[col])
                
                # 2. Moving averages for trend capture (appropriate for per-minute data)
                for window in [10, 15]:  # 10-min and 15-min windows for meaningful trends
                    feature_df[f'{col}_ma_{window}'] = feature_df[col].rolling(
                        window=window, min_periods=1
                    ).mean()
                    
                    # Deviation from moving average
                    feature_df[f'{col}_dev_{window}'] = (
                        feature_df[col] - feature_df[f'{col}_ma_{window}']
                    )
                
                # 3. Volatility measures (15-minute window for per-minute data)
                feature_df[f'{col}_volatility'] = feature_df[col].rolling(
                    window=15, min_periods=1
                ).std()
        
        self.logger.info(f"  âœ… Applied statistical transformations")
        return feature_df
    
    def create_scaling_target(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create sophisticated scaling target based on performance thresholds."""
        self.logger.info("ðŸŽ¯ Creating advanced scaling target variable...")
        
        feature_df = df.copy()
        
        # Initialize scaling action (0: scale down, 1: keep same, 2: scale up)
        feature_df['scaling_action'] = 1  # Default: keep same
        
        # Define performance thresholds based on research literature
        LATENCY_THRESHOLD_HIGH = 200  # ms - scale up threshold
        LATENCY_THRESHOLD_LOW = 50    # ms - scale down threshold
        LOAD_THRESHOLD_HIGH = 10      # requests/replica - scale up
        LOAD_THRESHOLD_LOW = 2        # requests/replica - scale down
        HEALTH_THRESHOLD = 0.95       # 95% health ratio minimum
        
        # Scale up conditions (any condition triggers scale up)
        scale_up_mask = pd.Series(False, index=feature_df.index)
        
        if 'avg_response_time' in feature_df.columns:
            scale_up_mask |= (feature_df['avg_response_time'] > LATENCY_THRESHOLD_HIGH)
        
        if 'requests_per_replica' in feature_df.columns:
            scale_up_mask |= (feature_df['requests_per_replica'] > LOAD_THRESHOLD_HIGH)
        
        if 'health_ratio' in feature_df.columns:
            scale_up_mask |= (feature_df['health_ratio'] < HEALTH_THRESHOLD)
        
        # Scale down conditions (all conditions must be met)
        scale_down_mask = pd.Series(True, index=feature_df.index)
        
        if 'avg_response_time' in feature_df.columns:
            scale_down_mask &= (feature_df['avg_response_time'] < LATENCY_THRESHOLD_LOW)
        
        if 'requests_per_replica' in feature_df.columns:
            scale_down_mask &= (feature_df['requests_per_replica'] < LOAD_THRESHOLD_LOW)
        
        if 'health_ratio' in feature_df.columns:
            scale_down_mask &= (feature_df['health_ratio'] >= HEALTH_THRESHOLD)
        
        # Apply scaling decisions
        feature_df.loc[scale_up_mask, 'scaling_action'] = 2
        feature_df.loc[scale_down_mask, 'scaling_action'] = 0
        
        action_counts = feature_df['scaling_action'].value_counts().sort_index()
        self.logger.info(f"  âœ… Scaling actions: Scale Down={action_counts.get(0, 0)}, "
                        f"Keep Same={action_counts.get(1, 0)}, Scale Up={action_counts.get(2, 0)}")
        
        return feature_df
    
    def select_optimal_features(self, df: pd.DataFrame) -> Tuple[List[str], Dict]:
        """Select the optimal features using advanced methods with correlation filtering."""
        self.logger.info(f"ðŸ§  Selecting optimal {self.target_features} features using advanced methods...")
        
        # Prepare feature matrix
        exclude_cols = ['scaling_action', 'timestamp'] if 'timestamp' in df.columns else ['scaling_action']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        X = df[feature_cols].copy()
        y = df['scaling_action']
        
        # Handle infinite and missing values
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        # Remove constant features
        constant_features = [col for col in feature_cols if X[col].std() == 0]
        if constant_features:
            self.logger.info(f"  Removing {len(constant_features)} constant features")
            feature_cols = [col for col in feature_cols if col not in constant_features]
            X = X[feature_cols]
        
        # Additional safety check for extreme values
        for col in feature_cols:
            # Cap extreme values at 99.9th percentile
            upper_bound = X[col].quantile(0.999)
            lower_bound = X[col].quantile(0.001)
            X[col] = X[col].clip(lower=lower_bound, upper=upper_bound)
        
        # Final check for any remaining infinite or NaN values
        X = X.replace([np.inf, -np.inf], 0).fillna(0)
        
        # Remove highly correlated features (NEW STEP)
        self.logger.info("  ðŸ” Removing highly correlated features...")
        correlation_matrix = X.corr().abs()
        
        # Find pairs of highly correlated features (>0.85)
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                if correlation_matrix.iloc[i, j] > 0.85:
                    high_corr_pairs.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        correlation_matrix.iloc[i, j]
                    ))
        
        # Remove redundant features (keep the first one in each pair)
        features_to_remove = set()
        for feat1, feat2, corr in high_corr_pairs:
            # Prioritize keeping base metrics over derived ones
            if '_dev_' in feat2 and '_dev_' not in feat1:
                features_to_remove.add(feat2)
            elif '_dev_' in feat1 and '_dev_' not in feat2:
                features_to_remove.add(feat1)
            elif 'memory_usage_mb' in feat1 and 'process_resident_memory_bytes' in feat2:
                # Keep memory_usage_mb (already in MB) over bytes version
                features_to_remove.add(feat2)
            elif 'memory_usage_mb' in feat2 and 'process_resident_memory_bytes' in feat1:
                features_to_remove.add(feat1)
            else:
                # Default: remove the second feature
                features_to_remove.add(feat2)
        
        if features_to_remove:
            self.logger.info(f"    Removing {len(features_to_remove)} highly correlated features:")
            for feat in features_to_remove:
                self.logger.info(f"      - {feat}")
            feature_cols = [col for col in feature_cols if col not in features_to_remove]
            X = X[feature_cols]
        
        # Method 1: Mutual Information
        self.logger.info("  ðŸ” Method 1: Mutual Information Analysis")
        mi_scores = mutual_info_regression(X, y, random_state=42)
        mi_ranking = sorted(zip(feature_cols, mi_scores), key=lambda x: x[1], reverse=True)
        
        # Method 2: Random Forest Feature Importance
        self.logger.info("  ðŸŒ² Method 2: Random Forest Feature Importance")
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X, y)
        rf_ranking = sorted(zip(feature_cols, rf.feature_importances_), key=lambda x: x[1], reverse=True)
        
        # Method 3: Correlation with target
        self.logger.info("  ðŸ“Š Method 3: Correlation Analysis")
        correlations = []
        for col in feature_cols:
            corr, p_value = pearsonr(X[col], y)
            correlations.append((col, abs(corr), p_value))
        corr_ranking = sorted(correlations, key=lambda x: x[1], reverse=True)
        
        # Method 4: Recursive Feature Elimination with Cross-Validation (Proper Method)
        self.logger.info("  ðŸŽ¯ Method 4: Recursive Feature Elimination with Cross-Validation")
        from sklearn.feature_selection import RFECV
        from sklearn.model_selection import StratifiedKFold
        
        # Use RFECV for optimal feature selection with cross-validation
        cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        rfe_cv = RFECV(
            estimator=RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
            step=1,  # Remove one feature at a time for precision
            cv=cv_splitter,
            scoring='neg_mean_squared_error',
            min_features_to_select=max(3, self.target_features//2),  # Don't go below minimum
            n_jobs=-1
        )
        rfe_cv.fit(X, y)
        rfe_selected = [feature_cols[i] for i, selected in enumerate(rfe_cv.support_) if selected]
        
        self.logger.info(f"    RFECV selected {len(rfe_selected)} optimal features")
        self.logger.info(f"    Optimal number of features: {rfe_cv.n_features_}")
        
        # Method 5: Statistical Significance Testing
        self.logger.info("  ðŸ“Š Method 5: Statistical Significance Testing")
        from scipy.stats import f_oneway
        
        stat_significant_features = []
        significance_scores = []
        
        for feature in feature_cols:
            # Group by scaling action and test if feature values differ significantly
            groups = [X[y == action][feature].values for action in [0, 1, 2] if (y == action).sum() > 0]
            
            if len(groups) >= 2 and all(len(group) > 0 for group in groups):
                try:
                    f_stat, p_value = f_oneway(*groups)
                    if p_value < self.significance_threshold:  # p < 0.05
                        stat_significant_features.append(feature)
                        significance_scores.append((feature, f_stat, p_value))
                except:
                    continue
        
        # Sort by F-statistic (higher = more discriminative)
        significance_scores.sort(key=lambda x: x[1], reverse=True)
        self.logger.info(f"    Found {len(stat_significant_features)} statistically significant features")
        
        # Method 6: Variance Inflation Factor (VIF) for Multicollinearity
        self.logger.info("  ðŸ” Method 6: Variance Inflation Factor Analysis")
        
        def calculate_vif(X_df):
            """Calculate VIF for each feature to detect multicollinearity."""
            from statsmodels.stats.outliers_influence import variance_inflation_factor
            vif_data = []
            
            for i in range(X_df.shape[1]):
                try:
                    vif = variance_inflation_factor(X_df.values, i)
                    if not np.isnan(vif) and not np.isinf(vif):
                        vif_data.append((X_df.columns[i], vif))
                except:
                    continue
            
            return vif_data
        
        try:
            # Calculate VIF for correlation-filtered features
            vif_results = calculate_vif(X)
            vif_results.sort(key=lambda x: x[1])  # Sort by VIF (lower is better)
            
            # Keep features with VIF < 10 (rule of thumb for low multicollinearity)
            low_vif_features = [feat for feat, vif in vif_results if vif < 10]
            self.logger.info(f"    {len(low_vif_features)} features with low multicollinearity (VIF < 10)")
            
            # Log high VIF features
            high_vif = [(feat, vif) for feat, vif in vif_results if vif >= 10]
            if high_vif:
                self.logger.warning(f"    High VIF features (>10): {high_vif[:5]}")
                
        except Exception as e:
            self.logger.warning(f"    VIF calculation failed: {e}")
            low_vif_features = feature_cols  # Fallback to all features
        
        # Advanced Ensemble Scoring with Multiple Methods
        feature_scores = {}
        total_weight = 0
        
        # Score from mutual information (weight: 0.25)
        mi_weight = 0.25
        for i, (feature, score) in enumerate(mi_ranking):
            feature_scores[feature] = feature_scores.get(feature, 0) + (len(feature_cols) - i) * mi_weight
        total_weight += mi_weight
        
        # Score from random forest (weight: 0.25)
        rf_weight = 0.25
        for i, (feature, score) in enumerate(rf_ranking):
            feature_scores[feature] = feature_scores.get(feature, 0) + (len(feature_cols) - i) * rf_weight
        total_weight += rf_weight
        
        # Score from correlation (weight: 0.15)
        corr_weight = 0.15
        for i, (feature, corr, p_val) in enumerate(corr_ranking):
            if p_val < self.significance_threshold:  # Only significant correlations
                feature_scores[feature] = feature_scores.get(feature, 0) + (len(feature_cols) - i) * corr_weight
        total_weight += corr_weight
        
        # Score from RFECV (weight: 0.20)
        rfecv_weight = 0.20
        for feature in rfe_selected:
            feature_scores[feature] = feature_scores.get(feature, 0) + len(feature_cols) * rfecv_weight
        total_weight += rfecv_weight
        
        # Score from statistical significance (weight: 0.10)
        stat_weight = 0.10
        for i, (feature, f_stat, p_val) in enumerate(significance_scores):
            feature_scores[feature] = feature_scores.get(feature, 0) + (len(significance_scores) - i) * stat_weight
        total_weight += stat_weight
        
        # Bonus for low multicollinearity (weight: 0.05)
        vif_weight = 0.05
        for feature in low_vif_features:
            if feature in feature_scores:
                feature_scores[feature] = feature_scores.get(feature, 0) + len(feature_cols) * vif_weight
        total_weight += vif_weight
        
        # Select top features
        final_ranking = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)
        selected_features = [feature for feature, score in final_ranking[:self.target_features]]
        
        # Create comprehensive analysis report
        analysis_report = {
            'selection_methods': {
                'mutual_information': dict(mi_ranking[:10]),
                'random_forest': dict(rf_ranking[:10]),
                'correlation': {feat: (corr, p_val) for feat, corr, p_val in corr_ranking[:10]},
                'rfecv_selected': rfe_selected,
                'statistical_significance': {feat: (f_stat, p_val) for feat, f_stat, p_val in significance_scores[:10]},
                'low_vif_features': low_vif_features[:20] if len(low_vif_features) > 20 else low_vif_features
            },
            'advanced_metrics': {
                'rfecv_optimal_features': len(rfe_selected),
                'statistically_significant_count': len(stat_significant_features),
                'low_multicollinearity_count': len(low_vif_features),
                'ensemble_weights': {
                    'mutual_information': mi_weight,
                    'random_forest': rf_weight,
                    'correlation': corr_weight,
                    'rfecv': rfecv_weight,
                    'statistical_significance': stat_weight,
                    'vif_bonus': vif_weight
                }
            },
            'final_scores': dict(final_ranking[:self.target_features]),
            'selected_features': selected_features
        }
        
        # Log comprehensive results
        self.logger.info(f"  âœ… Selected {len(selected_features)} optimal features using advanced methods")
        self.logger.info(f"  ðŸ“Š Method contributions:")
        self.logger.info(f"    - RFECV selected: {len(rfe_selected)} features")
        self.logger.info(f"    - Statistically significant: {len(stat_significant_features)} features")
        self.logger.info(f"    - Low multicollinearity: {len(low_vif_features)} features")
        self.logger.info(f"  ðŸŽ¯ Final selected features:")
        
        for i, feature in enumerate(selected_features, 1):
            score = final_ranking[i-1][1]
            self.logger.info(f"    {i:2d}. {feature:<40} (score: {score:.2f})")
        
        return selected_features, analysis_report
    
    def create_final_dataset(self, df: pd.DataFrame, selected_features: List[str]) -> pd.DataFrame:
        """Create the final dataset with selected features and comprehensive validation."""
        self.logger.info("ðŸŽ¯ Creating final dataset...")
        
        # Create final feature set
        final_cols = selected_features + ['scaling_action']
        if 'timestamp' in df.columns:
            final_cols = ['timestamp'] + final_cols
        
        final_df = df[final_cols].copy()
        
        # Comprehensive data cleaning
        self.logger.info("  ðŸ§¹ Performing comprehensive data cleaning...")
        
        # Handle infinite values first
        for feature in selected_features:
            # Replace infinite values with NaN
            final_df[feature] = final_df[feature].replace([np.inf, -np.inf], np.nan)
            
            # Fill NaN with median (robust to outliers)
            if final_df[feature].isnull().sum() > 0:
                median_val = final_df[feature].median()
                if pd.isna(median_val):  # If all values are NaN, use 0
                    median_val = 0
                final_df[feature] = final_df[feature].fillna(median_val)
                self.logger.debug(f"    Filled {feature} NaN values with {median_val}")
            
            # Cap extreme outliers (beyond 99.9th percentile)
            if final_df[feature].std() > 0:
                lower_bound = final_df[feature].quantile(0.001)
                upper_bound = final_df[feature].quantile(0.999)
                final_df[feature] = final_df[feature].clip(lower=lower_bound, upper=upper_bound)
        
        # Advanced data validation
        self.logger.info("  ðŸ” Performing final validation...")
        
        # Check for multicollinearity
        feature_corr_matrix = final_df[selected_features].corr()
        high_corr_pairs = []
        for i in range(len(feature_corr_matrix.columns)):
            for j in range(i+1, len(feature_corr_matrix.columns)):
                corr_val = abs(feature_corr_matrix.iloc[i, j])
                if corr_val > 0.8:  # High correlation threshold
                    high_corr_pairs.append((
                        feature_corr_matrix.columns[i], 
                        feature_corr_matrix.columns[j], 
                        corr_val
                    ))
        
        if high_corr_pairs:
            self.logger.warning(f"  âš ï¸ Found {len(high_corr_pairs)} highly correlated feature pairs:")
            for feat1, feat2, corr in high_corr_pairs:
                self.logger.warning(f"    {feat1} â†” {feat2}: {corr:.3f}")
        
        # Final check for any remaining infinite or NaN values
        for feature in selected_features:
            inf_count = np.isinf(final_df[feature]).sum()
            nan_count = final_df[feature].isnull().sum()
            if inf_count > 0 or nan_count > 0:
                self.logger.warning(f"  âš ï¸ {feature}: {inf_count} infinite, {nan_count} NaN values remaining")
                # Force clean any remaining issues
                final_df[feature] = pd.to_numeric(final_df[feature], errors='coerce').fillna(0)
        
        # Remove any rows where scaling_action is invalid
        initial_rows = len(final_df)
        final_df = final_df[final_df['scaling_action'].isin([0, 1, 2])]
        final_rows = len(final_df)
        
        if initial_rows != final_rows:
            self.logger.info(f"  ðŸ“Š Removed {initial_rows - final_rows} rows with invalid scaling actions")
        
        # If we have a very large dataset, we can optionally sample it down
        # but let's keep as much data as possible for better DQN training
        max_samples = 50000  # Increased from implicit ~1000 limit
        if len(final_df) > max_samples:
            self.logger.info(f"  ðŸ“Š Large dataset detected: {len(final_df):,} samples")
            self.logger.info(f"  ðŸ“Š Sampling to {max_samples:,} for computational efficiency")
            # Stratified sampling to maintain class distribution
            from sklearn.model_selection import train_test_split
            final_df, _ = train_test_split(
                final_df, 
                train_size=max_samples, 
                stratify=final_df['scaling_action'], 
                random_state=42
            )
            self.logger.info(f"  âœ… Sampled dataset maintains class distribution")
        
        self.logger.info(f"  âœ… Final dataset: {final_df.shape}")
        self.logger.info(f"  ðŸ“Š Action distribution: {dict(final_df['scaling_action'].value_counts().sort_index())}")
        
        return final_df
    
    def create_feature_scaler(self, df: pd.DataFrame, selected_features: List[str]) -> Tuple[any, Dict]:
        """Create an advanced feature scaler with detailed statistics."""
        self.logger.info("âš–ï¸ Creating feature scaler...")
        
        # Use RobustScaler for better handling of outliers
        scaler = RobustScaler()
        
        # Fit scaler on selected features
        X = df[selected_features].values
        scaler.fit(X)
        
        # Generate scaler statistics
        scaler_stats = {
            'scaler_type': 'RobustScaler',
            'features': selected_features,
            'n_features': len(selected_features),
            'n_samples': len(df),
            'feature_statistics': {}
        }
        
        for i, feature in enumerate(selected_features):
            scaler_stats['feature_statistics'][feature] = {
                'median': float(scaler.center_[i]),
                'scale': float(scaler.scale_[i]),
                'original_mean': float(df[feature].mean()),
                'original_std': float(df[feature].std()),
                'original_min': float(df[feature].min()),
                'original_max': float(df[feature].max())
            }
        
        self.logger.info(f"  âœ… Created RobustScaler for {len(selected_features)} features")
        return scaler, scaler_stats
    
    def save_results(self, df: pd.DataFrame, scaler: any, 
                    selected_features: List[str], analysis_report: Dict, 
                    scaler_stats: Dict, output_dir: Path) -> None:
        """Save results with comprehensive documentation."""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # Save dataset
        dataset_path = output_dir / "dqn_features.parquet"
        df.to_parquet(dataset_path, index=False)
        self.logger.info(f"  ðŸ’¾ Saved dataset: {dataset_path}")
        
        # Save scaler
        scaler_path = output_dir / "feature_scaler.gz"
        joblib.dump(scaler, scaler_path)
        self.logger.info(f"  ðŸ’¾ Saved scaler: {scaler_path}")
        
        # Save comprehensive metadata
        metadata = {
            'methodology': {
                'approach': 'Advanced multi-method feature selection',
                'target_features': self.target_features,
                'selection_methods': ['mutual_information', 'random_forest', 'correlation', 'rfe'],
                'statistical_validation': True,
                'scaler_type': 'RobustScaler'
            },
            'dataset_info': {
                'n_samples': len(df),
                'n_features': len(selected_features),
                'time_range': [str(df['timestamp'].min()), str(df['timestamp'].max())] if 'timestamp' in df.columns else None,
                'action_distribution': df['scaling_action'].value_counts().to_dict()
            },
            'selected_features': selected_features,
            'feature_analysis': analysis_report,
            'scaler_statistics': scaler_stats,
            'quality_metrics': {
                'missing_data_percentage': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,
                'feature_stability': {feat: float(df[feat].std() / (abs(df[feat].mean()) + 1e-6)) 
                                   for feat in selected_features}
            },
            'created_at': datetime.now().isoformat(),
            'research_grade': True
        }
        
        # Save metadata
        import json
        metadata_path = output_dir / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        self.logger.info(f"  ðŸ’¾ Saved metadata: {metadata_path}")
        
        # Create research summary
        self._create_research_summary(metadata, output_dir)
        
        self.logger.info("\n" + "="*70)
        self.logger.info("ðŸŽ“ ADVANCED DQN FEATURE ENGINEERING COMPLETE")
        self.logger.info("="*70)
        self.logger.info(f"ðŸ“Š Features: {len(selected_features)} (target: {self.target_features})")
        self.logger.info(f"ðŸ“ˆ Samples: {len(df):,}")
        self.logger.info(f"ðŸŽ¯ Action distribution: {dict(df['scaling_action'].value_counts().sort_index())}")
        self.logger.info(f"ðŸ“ Output: {output_dir}")
        self.logger.info("="*70)
    
    def _create_research_summary(self, metadata: Dict, output_dir: Path) -> None:
        """Create a research-grade summary document."""
        summary_md = f"""# Advanced DQN Feature Engineering - Research Summary

## Methodology Overview
This analysis employed advanced statistical and machine learning techniques to identify the {self.target_features} most critical features for DQN-based Kubernetes pod autoscaling.

## Advanced Feature Selection Methods Applied

### 1. Mutual Information Analysis (Weight: 25%)
- **Purpose**: Captures non-linear relationships between features and target
- **Method**: Information-theoretic measure of dependency
- **Top Features**: {list(metadata['feature_analysis']['selection_methods']['mutual_information'].keys())[:5]}

### 2. Random Forest Feature Importance (Weight: 25%)
- **Purpose**: Ensemble-based importance scoring with Gini impurity
- **Model**: 100 trees with random_state=42
- **Top Features**: {list(metadata['feature_analysis']['selection_methods']['random_forest'].keys())[:5]}

### 3. Correlation Analysis (Weight: 15%)
- **Purpose**: Linear relationship strength with target variable
- **Significance Level**: p < 0.05
- **Method**: Pearson correlation coefficient with p-value testing

### 4. Recursive Feature Elimination with Cross-Validation (Weight: 20%)
- **Purpose**: Optimal feature subset selection with cross-validation
- **Method**: RFECV with 5-fold stratified cross-validation
- **Base Estimator**: Random Forest (100 trees)
- **Selected Features**: {metadata['feature_analysis']['advanced_metrics']['rfecv_optimal_features']} features

### 5. Statistical Significance Testing (Weight: 10%)
- **Purpose**: ANOVA F-test for feature discrimination between scaling actions
- **Method**: One-way ANOVA with F-statistic ranking
- **Significant Features**: {metadata['feature_analysis']['advanced_metrics']['statistically_significant_count']} features (p < 0.05)

### 6. Variance Inflation Factor Analysis (Weight: 5%)
- **Purpose**: Multicollinearity detection and removal
- **Threshold**: VIF < 10 (low multicollinearity)
- **Low VIF Features**: {metadata['feature_analysis']['advanced_metrics']['low_multicollinearity_count']} features

## Final Selected Features

The following {len(metadata['selected_features'])} features were selected through ensemble ranking:

"""
        for i, feature in enumerate(metadata['selected_features'], 1):
            summary_md += f"{i:2d}. `{feature}`\n"
        
        summary_md += f"""

## Dataset Statistics
- **Total Samples**: {metadata['dataset_info']['n_samples']:,}
- **Features**: {metadata['dataset_info']['n_features']}
- **Time Range**: {metadata['dataset_info']['time_range'][0] if metadata['dataset_info']['time_range'] else 'N/A'} to {metadata['dataset_info']['time_range'][1] if metadata['dataset_info']['time_range'] else 'N/A'}
- **Missing Data**: {metadata['quality_metrics']['missing_data_percentage']:.3f}%

## Scaling Action Distribution
- **Scale Down**: {metadata['dataset_info']['action_distribution'].get(0, 0)} ({metadata['dataset_info']['action_distribution'].get(0, 0)/metadata['dataset_info']['n_samples']*100:.1f}%)
- **Keep Same**: {metadata['dataset_info']['action_distribution'].get(1, 0)} ({metadata['dataset_info']['action_distribution'].get(1, 0)/metadata['dataset_info']['n_samples']*100:.1f}%)
- **Scale Up**: {metadata['dataset_info']['action_distribution'].get(2, 0)} ({metadata['dataset_info']['action_distribution'].get(2, 0)/metadata['dataset_info']['n_samples']*100:.1f}%)

## Quality Assurance
- âœ… **Statistical Significance**: All correlations tested at p < 0.05
- âœ… **Multicollinearity Check**: Features tested for high correlation (>0.8)
- âœ… **Outlier Handling**: RobustScaler used for outlier-resistant normalization
- âœ… **Missing Data**: Advanced imputation and validation
- âœ… **Feature Stability**: Coefficient of variation calculated for all features

## Research Impact
This advanced feature engineering approach provides:
1. **Reduced Dimensionality**: From 100+ raw metrics to {len(metadata['selected_features'])} optimal features
2. **Statistical Rigor**: Multiple validation methods ensure feature quality
3. **Domain Knowledge**: Features aligned with autoscaling theory
4. **Reproducibility**: Comprehensive documentation and metadata

---
*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} using advanced statistical methods*
"""
        
        with open(output_dir / "research_summary.md", 'w') as f:
            f.write(summary_md)
    
    def run_pipeline(self, output_dir: str = "dqn_data") -> None:
        """Execute the complete feature engineering pipeline."""
        self.logger.info("ðŸš€ Starting Advanced DQN Feature Engineering Pipeline")
        self.logger.info("="*70)
        
        try:
            # Step 1: Load and prepare data
            raw_df = self.load_and_prepare_data()
            
            # Step 2: Create time-series features
            ts_df = self.create_time_series_features(raw_df)
            
            # Step 3: Engineer domain-specific features
            domain_df = self.engineer_domain_features(ts_df)
            
            # Step 4: Apply statistical transformations
            transformed_df = self.apply_statistical_transformations(domain_df)
            
            # Step 5: Create scaling target
            target_df = self.create_scaling_target(transformed_df)
            
            # Step 6: Select optimal features
            selected_features, analysis_report = self.select_optimal_features(target_df)
            
            # Step 7: Create final dataset
            final_df = self.create_final_dataset(target_df, selected_features)
            
            # Step 8: Create feature scaler
            scaler, scaler_stats = self.create_feature_scaler(final_df, selected_features)
            
            # Step 9: Save results
            self.save_results(
                final_df, scaler, selected_features, 
                analysis_report, scaler_stats, Path(output_dir)
            )
            
            self.logger.info("âœ… Pipeline completed successfully!")
            
        except Exception as e:
            self.logger.error(f"âŒ Pipeline failed: {e}")
            raise

def main():
    """Main execution function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Advanced DQN Feature Engineering with Consumer Pod Focus")
    parser.add_argument("--data-dir", type=str, default="prometheus_data",
                        help="Directory containing Prometheus CSV files")
    parser.add_argument("--output-dir", type=str, default="dqn_data",
                        help="Output directory for processed features")
    parser.add_argument("--target-features", type=int, default=8,
                        help="Number of features to select for DQN")
    parser.add_argument("--prometheus-url", type=str, default="http://localhost:9090",
                        help="Prometheus server URL for dynamic metric discovery")
    
    args = parser.parse_args()
    
    # Initialize feature engineer
    engineer = AdvancedDQNFeatureEngineer(
        data_dir=Path(args.data_dir),
        target_features=args.target_features,
        prometheus_url=args.prometheus_url
    )
    
    # Run the pipeline
    engineer.run_pipeline(args.output_dir)

if __name__ == "__main__":
    main() 